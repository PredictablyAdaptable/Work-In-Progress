{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised ML\n",
    "\n",
    "Table of Content\n",
    "\n",
    "1. Supervised vs. Unsupervised\n",
    "2. Regression \n",
    "3. Gradient Decent\n",
    "4. Multiple Linear Regression\n",
    "5. Gradient Decent In Practice\n",
    "6. Classification\n",
    "7. Logistic Regression\n",
    "    1. Cost Function\n",
    "    2. Gradient Decent in Logistic Regression\n",
    "8. Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "1. X- Y\n",
    "2. Give learning algo correctly labelled Y variables\n",
    "3. Regression -> predict the correct number out of an infinately many possible numbers (continuous outcome variable)\n",
    "4. Classification -> logistic regression. Finite number of outcome variables (categorical, nominal outcome variable)\n",
    "\n",
    "Nomenclature -> output(Y) -> class, category, output, outcome etc\n",
    "\n",
    "#### Unsupervised Learning\n",
    "1. There is no indication of correctly labelled Y variables, data only comes with features algo is used to find structure\n",
    "2. Can be use dfor EDA, such as find some structure/pattern in data\n",
    "    1. Clustering\n",
    "        1. exmaple -> google news (finding similar news), genetic similarities\n",
    "    2. Anomoaly Detection\n",
    "        1. find unusual datapoints\n",
    "    3. Dimentionality Reduction\n",
    "        1. compress data using fewer nubmer of variables without loosing a lot of valuable info\n",
    "\n",
    "\n",
    "- More Nomenclature\n",
    "1. (x,y) -> single training example, x is the input(s) and y is the output. \n",
    "2. (x^(i), y^(i)) -> ith training example\n",
    "\n",
    "\n",
    "\n",
    "#### Cost Function\n",
    "1. its the error between y_hat and y calculated as (yhat - y)**2\n",
    "2. in the case of linreg it would be the r**2\n",
    "3. basically gives the model fit\n",
    "\n",
    "#### Choosing best fit model with cost function intuition\n",
    "1. a model has parameters i.e. lin reg yhat = wx + b\n",
    "2. a simplified model can be drawn for intuition where yhat = wx, where\n",
    "    1. x is the input variable\n",
    "    2. y is the output variable\n",
    "    3. w is the model params\n",
    "3. cost function for simplified model is Mean Squared Error\n",
    "    1. j(w) = ((sum from i =1 to m)(fw(xi)-y(i))**2)/2m \n",
    "\n",
    "4. Gettting Best Fit\n",
    "    1. goal of best fit is to min(j)\n",
    "    2. as there is only 1 param that can be varied which is Y\n",
    "    3. the value of y is a function fo w so change w to min j\n",
    "\n",
    "\n",
    "\n",
    "#### Gradient Decent\n",
    "1. finding the smallest cost function\n",
    "2. goal of gradient decent\n",
    "    1. you have some function\n",
    "    2. j(w_1, ..., W_n, b) you want to change the parameters w and b to min J\n",
    "\n",
    "#### How algo works\n",
    "1. you start at point 0,0,0\n",
    "2. then you go to the direction of the steepest decent until you find the local minima\n",
    "3. you can vary the algo i.e. 2nd steepest direction to find different local minima \n",
    "\n",
    "#### Mathmatics of Gradient Decent\n",
    "1. Adjusting for W\n",
    "    1. w_new => w_old - a * d/dw * j(w,b)\n",
    "        1. the w_old(starting point) is adjusted a small bit by a\n",
    "            in this case a is the learning rate\n",
    "        2. d/dw * j(w,b) -> derivative of cost function j, basically finding the steapest dirction \n",
    "    \n",
    "2. adjusting for b\n",
    "    1. b_new <- b_old - a* d/db * j(w,b)\n",
    "        1. same process as w. \n",
    "        2. important to note, you use the same alpha, to produce w_new, and use the old w, to create the b_new. do not use new_w to adjust old_b\n",
    "\n",
    "3. When cost function reaches minimum\n",
    "    1. as in the derivative (rate of decrease) = 0, the cost function does not change\n",
    "4. Important to Note\n",
    "    1. as parameters get closer to local minimum, the gradient decent will automatically take smaller steps\n",
    "        1. this is because alpha is multiplied by the slope of the tangent which reduces as the algo decends\n",
    "\n",
    "\n",
    "\n",
    "Understanding Multi Regression\n",
    "1. F_w,b(x) = w_1 * X_1 + ... w_n * X_n +b\n",
    "    1. as an example lets say\n",
    "        1. dependent variable F_w,b(x) or Y, i.e. house price\n",
    "        2. independent variable X_1, ... X_n i.e. sqft, number of windows\n",
    "        3. Then to interpret this model\n",
    "            1. b-> base price, if all other feature variables = 0\n",
    "            2. W -> weights of the feature, importance of each feature\n",
    "\n",
    "\n",
    "Vectorization\n",
    "1. makes writing code faster, and running code faster\n",
    "2. vector transformations allow you to process values in vecotr to be computed in parallel, making computer faster\n",
    "\n",
    "Feature Scaling\n",
    "1. Transforing feature scales so you compare apples to apples\n",
    "    1. it also makes model weights much easier to interperate. Weights importance can now be ranked if feature scale is the same\n",
    "\n",
    "Methods of Feature Scaling\n",
    "1. dividing by the maximum Xn/max(X)\n",
    "2. Mean Normalization \n",
    "    1. each feature is recentered around the mean. mean become 0\n",
    "    2. X_new = (x_old - avg)/(max-min)\n",
    "3. Z-Score Normalization (standardization)\n",
    "    1. x_new = (x_old-avg)/std\n",
    "\n",
    "\n",
    "Gradient Decent Learning Rate\n",
    "1. use a graph search and pick the one that gives the fastist/consistant learning rate\n",
    "\n",
    "Feature Engineering \n",
    "1. combining/transforming existing features based on your intition so the algo can learn more efficiently \n",
    "\n",
    "Polynomial Regression\n",
    "1. Y= w_1*x + w_2*x**2... +b\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "1. Sigmoid Function\n",
    "    this function essentially is what transforms lin-reg to log-reg\n",
    "\n",
    "2. Interpretation of Logi-Reg\n",
    "    1. the output is the 'probability' that the class is 1\n",
    "\n",
    "\n",
    "Nomanclature\n",
    "1. loss function -> difference between predicted and actual at a single point\n",
    "2. cost function -> avg diff between actuall and pred\n",
    "\n",
    "The Problem with Overfitting\n",
    "1. Model Underfit-> High Bias, the model is missing clear patterns that it's not capturing\n",
    "2. Good Model fit -> Generalization, generalizable model. Model generalizes to other new data\n",
    "3. Model Overfit -> High Variance, model fits training set too well, can not generalize to new data\n",
    " \n",
    "Addressing Overfitting\n",
    "1. Collect more training data\n",
    "2. Feature selection\n",
    "    1. Select only a handful of features that intuitively make sense out of all the features\n",
    "    2. If there are too many features, and not enough observations, higher chances of overfitting\n",
    "3. Regularization\n",
    "    1. You don't get rid of features, but encourage the algo to shrink the weight of some of the features so the features that have smaller weights do not have a large impact to the model.\n",
    "    2. Lets you keep all your features\n",
    "\n",
    "\n",
    "\n",
    "changes\n",
    "i need to add changes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
