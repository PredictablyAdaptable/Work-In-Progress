{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Algorithms\n",
    "\n",
    "Table of Content\n",
    "\n",
    "1. Neural Networks\n",
    "    1. Intuition\n",
    "    2. Model\n",
    "    3. Tensorflow\n",
    "    4. Python Implementation\n",
    "\n",
    "2. Vectorization\n",
    "\n",
    "3. Neural Network Training\n",
    "    1. Activation Function\n",
    "    2. Multiclass Classification\n",
    "    3. Advanced Optimization & Layering\n",
    "\n",
    "4. Back Propagation\n",
    "\n",
    "5. Bias & Variance\n",
    "\n",
    "6. ML Development Process\n",
    "\n",
    "7. Skewed Dataset\n",
    "\n",
    "8. Decision Trees\n",
    "    1. Decision Tree Learning\n",
    "    2. Tree Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "Nomenclature\n",
    "1. root node -> first node\n",
    "2. decision nodes -> nodes after root \n",
    "3. purity -> % of output classification is classified correctly \n",
    "\n",
    "\n",
    "Learning Process of Decision Trees\n",
    "1. decision 1 -> decisions to make for trees\n",
    "    1. how to choose which feature to use at which node?\n",
    "        1. max purity\n",
    "2. decision 2 -> when do you stop splitting? (model hyper params)\n",
    "    1. when node is 100% one class\n",
    "    2. when splitting a node will result in the tree exceeds max depth\n",
    "    3. when improvments in purity score are below threshold \n",
    "    4. when number of examples in a node is below a threshold\n",
    "\n",
    "\n",
    "Entropy\n",
    "1. impurity\n",
    "2. a function that measures impurity\n",
    "3. you use it to eval how well a node classifies data\n",
    "4. intuition\n",
    "    1. whithin a set there are 2 categories, pure/impure\n",
    "    2. a ratio is created to determine weather the set is impure (50/50 split) or pure (100/0, 0/100 split)\n",
    "    3. so this ratio is in essenece the p that goes in to the entropy function\n",
    "\n",
    "5. entropy is denoted as H(p) where p= fraction of examples that are true\n",
    "6. function is\n",
    "    1. H(p) = -p* log(p) - (1-p) * log(1-p)\n",
    "        0 is pure, 1 is impure. \n",
    "\n",
    "Getting from Entropy to Information Gain\n",
    "1. quntification on how effective a node is splitting the data so that it becomes more/less pure\n",
    "2. intution\n",
    "    1. you have the entropy value of the data before a node splits, and the entropy value for the subset of data after the split occurs.\n",
    "    2. you take the weighted entropy value of the post split values and compare it to the pre-split value\n",
    "\n",
    "3. Mathmatically \n",
    "    1. information gain = H(p_root) - w_left * H(p_left) + w_right * H(p_right)\n",
    "    2. so in essence \n",
    "        1. information gain = root branch entropy - weighted avg sub branch entropy\n",
    "        2. since entropy 1 is impure, and 0 means pure, improvment in entropy (information gain)\n",
    "            1. information gain > 0\n",
    "\n",
    "Creating bools rom different variables\n",
    "1. one-hot encoding\n",
    "    1. bascially create multiple bool from cat variable, by doing \n",
    "        1. cat 1 = true, not cat 1 = false. 2nd bool = cat 2 = true, not cat 2 = false repeat to n categories\n",
    "        2. in python `df = pd.getdummies(data = df, prefix = cat_variables, coumns = cat_variables)\n",
    "\n",
    "2. continuous valued features\n",
    "    1. bascially create a numeric threshold\n",
    "    2. true if greater/lesser than threshold, vice versa\n",
    "\n",
    "Regression Trees\n",
    "1. x variables is are bools, but y is float\n",
    "2. split using nodes, then leaf node is the average y value within the leaf dataset\n",
    "3. goal of regression tree -> each split results in a smaller variance within the subset\n",
    "\n",
    "Info Gain in Regression Tress\n",
    "1. intuition -> same as bin classification except instead of using entropy use variance. \n",
    "    1. Information gain = root_var - (w_left * left_subset_var + w_right * right_subset_var)\n",
    "        1. w_left & w_right -> count(subset)/count(root)\n",
    "\n",
    "\n",
    "Using Multiple Decision Trees\n",
    "1. weakness of single decision trees\n",
    "    1. highly sensitive\n",
    "    2. tree ensamble \n",
    "        1. multiple decision trees to address sensitivity\n",
    "\n",
    "2. the Ensambles way\n",
    "    1. multiple trees with different methods of creating the tree\n",
    "    2. each model will predict based on input x\n",
    "    3. then you get trees to vote\n",
    "    4. take the average vote\n",
    "\n",
    "\n",
    "Building Tree Ensemble\n",
    "1. Bagged Decision Trees \n",
    "    1. Bootstrapping Data\n",
    "        1. create multiple training set from original dataframe\n",
    "        2. sample with replacment and sample n observations where n = len(original_data)\n",
    "        3. n sample dataset = n trees in ensemble\n",
    "    2. determining number of trees\n",
    "        1. typically between 64-128, usually 100\n",
    "        2. dim return after 130\n",
    "\n",
    "2. Random Forests\n",
    "    1. Bagged Decision Trees + random feature selection\n",
    "    2. Random Freature selction\n",
    "        1. 1 more layer of complexity, where you don't use all feature variables, but n sample features (without replacment) \n",
    "        2. random feature selection occurs on every level of node. meaning choose random k features at root, then another random feature selection in next node etc.\n",
    "    3. done on every tree\n",
    "    4. each tree is independent \n",
    "\n",
    "3. XGBoost\n",
    "    1. random forest + reinforcement learning\n",
    "    2. Reinforcement Learning\n",
    "        1. after creating the first tree, you modify the probability of which observations to choose in the bootstrap of sampled dataset for tree creation\n",
    "        2. make observations that are missclassified more likely to be sampled in random sampling in boostrapped datset\n",
    "    3. Trees are dependent of previous tree\n",
    "    \n",
    "4. Modeling Process of XGBoost\n",
    "    1. split train test set\n",
    "    2. split train set to fit and eval\n",
    "    3. so the overall data split structure\n",
    "        1. main\n",
    "            1. test (10-30%)\n",
    "            2. train (70-90%)\n",
    "                1. fit (10-20% of train)\n",
    "                2. eval (80-90% of train)\n",
    "\n",
    "    4. what the split does\n",
    "        1. test -> test the entire, once the model is constructed\n",
    "        2. train -> data used to train the data\n",
    "            1. fit -> data actually used to fit (train) the model. use this data to min cost. \n",
    "            2. eval -> smaller subset during training to eval model performance for each iteration. \n",
    "                1. this exists because it is an ensamble of multiple trees\n",
    "                2. after each iteration of tree creation the aggregate tree ensemble's performance is tested against the evaluation set\n",
    "                3. if the performance on eval set improves, training continues. if performance stagnates/worsens, model stops. \n",
    "\n",
    "    5. Eval set in practice\n",
    "        1. Letâ€™s say you are tuning the model and testing different hyperparameters like learning rate, tree depth, and regularization:\n",
    "            1. You train the model with a learning rate of 0.3, max depth of 6, and regularization strength of 1.0.\n",
    "            2. After each boosting iteration, you evaluate the performance on the eval set.\n",
    "            3. If you see that the performance on the training set is improving while the eval set starts deteriorating, this indicates overfitting. You might then:\n",
    "            4. Decrease the learning rate to 0.1 to slow down the learning process.\n",
    "            5. Reduce the max tree depth to 4 to prevent overfitting.\n",
    "            6. Increase regularization (e.g., lambda from 1.0 to 2.0) to reduce model complexity.\n",
    "\n",
    "##### In Python\n",
    "\n",
    "Starting from the Training dataset\n",
    "1. `N = int(len(X_train)*0.8)` # i.e. 80% the count of 80% of observations is saved as value N\n",
    "2. `X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:N], X_train[N:], y_train[N:], y_train[:N]` # train data is split between fit set and eval set, with observations below index value N being part of fit set and above N being part of eval set\n",
    " \n",
    "Modeling\n",
    "`xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1, verbosity = 1, random_state = RANDOM_STATE)`\n",
    "`xgb_model.fit(X_train_fit, y_train_fit, eval_set[(X_train_eval, y_train_eval)], early_stopping_rounds = 10)`\n",
    "1.  when fitting the model with the data, you need to feed it 4 datasets, X_fit & eval, y_fit & eval.\n",
    "2.  the early_stopping_rounds parameter helps to stop the model training if its eval metric is no longer improving on the validation set.\n",
    " \n",
    "Model Results\n",
    "1. Gives you model with the best hyper parameters.\n",
    "2. `gb_model.best_iteration` # gives you the optimized number of trees in the forest.\n",
    "3. Then you do a final test of model against the test set\n",
    " \n",
    " \n",
    "Classification Trees\n",
    "`from xgboost import XGBClassifier`\n",
    "`model = XGBClassifier()`\n",
    "`model.fit(X_train, y_train)`\n",
    "`y_pred = model.predict(X_test)`\n",
    "\n",
    "Regressed Trees\n",
    "`from xgboost import XGBRegressor`\n",
    "`model = XGBRegressor()`\n",
    "`model.fit(X_train, y_train)`\n",
    "`y_pred = model.predict(X_test)`\n",
    " \n",
    "When to use Decision Trees\n",
    "1. When using Structured(Tabular) Data not on unstructured data\n",
    "2. When you need to train a model quickly\n",
    "3. Small decision trees are human interpretable\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
